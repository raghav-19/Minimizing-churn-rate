# -*- coding: utf-8 -*-
"""minimize_churn_rate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KnnsUJM8RvyFsGF78ungrbK6MHu3e7H3

##Importing Library and Reading Data
"""

import numpy as np;
import pandas as pd;
import matplotlib.pyplot as plt;
import seaborn as sns;

df=pd.read_csv("/content/drive/MyDrive/Dataset/churn_data.csv")
df.head()

df.shape

"""##Treating Missing Columns"""

df.isnull().sum()

df['age'].dtype=='float64'

#Finding numerical columns with missing value
mis_cols=[]
for col in df.columns:
  if df[col].isnull().sum()>0 and (df[col].dtype=='float64' or df[col].dtype=='int64'):
    mis_cols.append(col)
mis_cols

def treat_missing_column(dataFrame, mis_cols, what_to_do):
    if what_to_do == 'mean':
        from sklearn.impute import SimpleImputer
        imputer=SimpleImputer(missing_values=np.nan, strategy='mean')
        imputer.fit(dataFrame[mis_cols])
        dataFrame[mis_cols]=imputer.transform(dataFrame[mis_cols])
        return dataFrame
    elif what_to_do == 'drop':
        dataFrame.drop(columns=mis_cols,inplace=True)
        return dataFrame
df=treat_missing_column(df,mis_cols,'mean')

"""##EDA"""

df.describe()

"""###Plotting Histograms and Pie Charts"""

def plot_hist_each_col(dataFrame):
    col=np.floor(np.sqrt(dataFrame.shape[1]))
    row=int(dataFrame.shape[1]/col)+1
    plt.figure(figsize=(5*col,5*row));
    plt.suptitle("Histogram of Values",fontsize=20);
    for i in range(0,dataFrame.shape[1]):
        plt.subplot(row,col,i+1);
        f=plt.gca();
        f.set_title(dataFrame.columns[i]);
        vals=dataFrame.iloc[:,i].nunique();
        plt.hist(dataFrame.iloc[:,i],bins=vals);
    plt.tight_layout(rect=[0,0.03,1,0.95]);
    #plt.savefig('Histogram.jpg')

plot_hist_each_col(df.drop(columns=['user','churn']))

def plot_pie_each_col(dataFrame):
    col=np.floor(np.sqrt(dataFrame.shape[1]))
    row=int(dataFrame.shape[1]/col)+1
    plt.figure(figsize=(5*col,5*row));
    plt.suptitle("Pie Chart of Values",fontsize=20);
    for i in range(0,dataFrame.shape[1]):
        plt.subplot(row,col,i+1);
        f=plt.gca();
        f.set_title(dataFrame.columns[i]);
        values=dataFrame.iloc[:,i].value_counts(normalize=True).values;
        indexs=dataFrame.iloc[:,i].value_counts(normalize=True).index;
        plt.pie(values, labels=indexs);
    plt.tight_layout(rect=[0,0.03,1,0.95]);
    #plt.savefig('Pie_Chart.jpg')

plot_pie_each_col(df[['housing', 'app_downloaded', 'web_user', 'app_web_user', 'ios_user', 'android_user', 'payment_type', 'waiting_4_loan',
                      'cancelled_loan', 'received_loan', 'rejected_loan', 'zodiac_sign', 'left_for_two_month_plus', 'left_for_one_month', 'is_referred']])

"""###Analyzing Binary value column for minority value"""

uneven_col=['app_downloaded', 'waiting_4_loan', 'cancelled_loan', 'received_loan', 'rejected_loan', 'left_for_one_month'];
for col in uneven_col:
    minor=df[col].value_counts().index[1];
    print(col, minor)
    print(df[df[col]==minor]['churn'].value_counts())

"""###Correlation with output and correlation matrix"""

df.drop(columns=['user','churn','housing','payment_type','zodiac_sign']).corrwith(df['churn']).plot.bar(figsize=(20,5),
                                                                                                        title='Correlation with target variable')

plt.figure(figsize=(20,16))
sns.heatmap(df.drop(columns=['user','churn']).corr(),annot=True)

df.drop(columns=['app_web_user'],inplace=True)

"""##Data Preparation

###One Hot Encoding
"""

df=pd.get_dummies(df)
df.columns

#Removing columns so that they remain independent features
df.drop(columns=['housing_na','payment_type_na','zodiac_sign_na'],inplace=True)

"""###Splitting train and test dataset"""

X=df.drop(columns=['user','churn'])
y=df['churn']

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)

"""###Feature Scaling"""

from sklearn.preprocessing import StandardScaler
sc_x=StandardScaler()
X_train=sc_x.fit_transform(X_train)
X_test=sc_x.transform(X_test)

"""##Model Building"""

#Confusion Matrix Visualization
from sklearn.metrics import confusion_matrix
def heatmap_confusion_matrix(y_true,y_pred):
    cm=confusion_matrix(y_true,y_pred)
    group_names = ['True Neg','False Pos','False Neg','True Pos']
    group_counts = ['{0:0.0f}'.format(value) for value in cm.flatten()]
    labels = [f'{v1}\n{v2}' for v1, v2 in zip(group_names,group_counts)]
    labels = np.asarray(labels).reshape(2,2)
    plt.figure(figsize=(6,4))
    sns.set(font_scale=1.2)
    sns.heatmap(cm,annot=labels,fmt='',cmap='Blues')
    plt.xlabel("Predicted Value")
    plt.ylabel("Actual Value")
    plt.show()

#Classification Metrics
from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score
def classification_metrics(y_true,y_predict,y_probability):
    print("Accuracy Score  : ",round(accuracy_score(y_true,y_predict),3))
    print("Precision Score : ",round(precision_score(y_true,y_predict),3))
    print("Recall Score    : ",round(recall_score(y_true,y_predict),3))
    print("F1 Score        : ",round(f1_score(y_true,y_predict),3))
    print("roc_auc_score   : ",round(roc_auc_score(y_true,y_probability),3))

#Evaluate Binary Classification Model
def evaluate_model(model,train_X,test_X,train_y,test_y,name):
    print(name)
    model.fit(train_X,train_y)
    m_pred=model.predict(test_X)
    m_prob=model.predict_proba(test_X)[:,1]
    heatmap_confusion_matrix(test_y,m_pred)
    classification_metrics(test_y,m_pred,m_prob)

from sklearn.linear_model import LogisticRegression
log_model=LogisticRegression()
evaluate_model(log_model, X_train, X_test, y_train, y_test, "Logistic Regression")

from sklearn.ensemble import RandomForestClassifier
rd_model=RandomForestClassifier()
evaluate_model(rd_model, X_train, X_test, y_train, y_test, "Random Forest Classifier")

from xgboost import XGBClassifier
xgb_model=XGBClassifier()
evaluate_model(xgb_model, X_train, X_test, y_train, y_test, "XG Boost Classifier")

"""###Cross Validation in Random Forest"""

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator=rd_model, X=X_train, y=y_train, cv=10)
accuracies.mean()

accuracies

"""###Feature Importance in Random forest"""

#Creating Plot of Feature Importance
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
def plot_feature_importance(feature_val,col,model_name):
    feature_imp = pd.DataFrame(sorted(zip(feature_val,col)), columns=['Value','Feature'])
    plt.figure(figsize=(16, 10))
    sns.barplot(x="Value", y="Feature", data=feature_imp.sort_values(by="Value", ascending=False))
    plt.title(model_name+' Feature importance')
    plt.tight_layout()
    plt.show()

rd_model.feature_importances_

X_col=df.drop(columns=['user','churn']).columns
plot_feature_importance(rd_model.feature_importances_, X_col, "Random Forest")

"""##Feature Selection"""

from sklearn.feature_selection import RFE
my_model=RandomForestClassifier()
rfe=RFE(my_model, 20)
rfe=rfe.fit(X_train,y_train)
rfe.support_

X_col[rfe.support_]

X_train=X_train[:,rfe.support_].copy()
X_test =X_test[: ,rfe.support_].copy()

my_model=RandomForestClassifier()
evaluate_model(my_model, X_train, X_test, y_train, y_test, "Random Forest Classifier")